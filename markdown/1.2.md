# §1.2 Elementary Algebra

:::{note}
**Referenced by:**

Erratum (V1.2.0) §1.2

**See also:**

Annotations for Ch.1
:::


## §1.2(i) Binomial Coefficients

:::{note}
**Defines:**

$\genfrac{(}{)}{0.0pt}{}{\NVar{m}}{\NVar{n}}$ : binomial coefficient

**Keywords:**

[binomial coefficients](http://dlmf.nist.gov/search/search?q=binomial%20coefficients) , [binomials](http://dlmf.nist.gov/search/search?q=binomials) , [definition](http://dlmf.nist.gov/search/search?q=definition)

**Notes:**

See Graham et al. ([1994](./bib/G.html#bib974 "Concrete Mathematics: A Foundation for Computer Science"), pp. 157–174). For ( 1.2.1 ), see also Chrystal ([1959b](./bib/C.html#bib501 "Algebra: An Elementary Textbook for the Higher Classes of Secondary Schools and for Colleges"), p. 8). For ( 1.2.2 ) and ( 1.2.7 ), see Chrystal ([1959a](./bib/C.html#bib500 "Algebra: An Elementary Textbook for the Higher Classes of Secondary Schools and for Colleges"), pp. 62–70).

**Referenced by:**

§1.1 , §26.3(i) , §26.3(iv) , Erratum (V1.0.11) for Subsection 1.2(i) , Erratum (V1.0.11) for Clarifications , Erratum (V1.0.5) for Subsection 1.2(i) , Erratum (V1.0.7) for Usability

**Clarification (effective with 1.0.11):**

A sentence has been added after ( 1.2.1 ) to refer to ( 1.2.6 ) as the definition of the binomial coefficient $\genfrac{(}{)}{0.0pt}{}{z}{k}$ when $z$ is complex. As a notational clarification, wherever $n$ appeared originally in ( 1.2.6 )–( 1.2.9 ), it has been replaced by $z$ .

**Addition (effective with 1.0.7):**

The cross-reference to § [26.3](./26.3.md "§26.3 Lattice Paths: Binomial Coefficients ‣ Properties ‣ Chapter 26 Combinatorial Analysis") has been added at the end of this subsection. *Suggested 2013-11-25 by Howard Cohl*

**Errata (effective with 1.0.5):**

The condition for ( 1.2.2 ), ( 1.2.4 ), and ( 1.2.5 ) was corrected. These equations are true only if $n$ is a positive integer. Previously $n$ was allowed to be zero. *Reported 2011-08-10 by Michael Somos*

**See also:**

Annotations for §1.2 and Ch.1
:::

In ( 1.2.1 ) and ( 1.2.3 ) $k$ and $n$ are nonnegative integers and $k\leq n$ . In ( 1.2.2 ), ( 1.2.4 ), and ( 1.2.5 ) $n$ is a positive integer. See also § 26.3(i) .


<a id="E1"></a>
$$
\genfrac{(}{)}{0.0pt}{}{n}{k}=\frac{n!}{(n-k)!k!}=\genfrac{(}{)}{0.0pt}{}{n}{n-k}. \tag{1.2.1}
$$

For complex $z$ the binomial coefficient $\genfrac{(}{)}{0.0pt}{}{z}{k}$ is defined via ( 1.2.6 ).


### Binomial Theorem

:::{note}
**Keywords:**

[binomial theorem](http://dlmf.nist.gov/search/search?q=binomial%20theorem)

**See also:**

Annotations for §1.2(i) , §1.2 and Ch.1
:::


<a id="E2"></a>
$$
(a+b)^{n}=a^{n}+\genfrac{(}{)}{0.0pt}{}{n}{1}a^{n-1}b+\genfrac{(}{)}{0.0pt}{}{n}{2}a^{n-2}b^{2}+\dots+\genfrac{(}{)}{0.0pt}{}{n}{n-1}ab^{n-1}+b^{n}. \tag{1.2.2}
$$


<a id="E3"></a>
$$
\genfrac{(}{)}{0.0pt}{}{n}{0}+\genfrac{(}{)}{0.0pt}{}{n}{1}+\dots+\genfrac{(}{)}{0.0pt}{}{n}{n}=2^{n}. \tag{1.2.3}
$$


<a id="E4"></a>
$$
\genfrac{(}{)}{0.0pt}{}{n}{0}-\genfrac{(}{)}{0.0pt}{}{n}{1}+\dots+(-1)^{n}\genfrac{(}{)}{0.0pt}{}{n}{n}=0. \tag{1.2.4}
$$


<a id="E5"></a>
$$
\genfrac{(}{)}{0.0pt}{}{n}{0}+\genfrac{(}{)}{0.0pt}{}{n}{2}+\genfrac{(}{)}{0.0pt}{}{n}{4}+\dots+\genfrac{(}{)}{0.0pt}{}{n}{\ell}=2^{n-1}, \tag{1.2.5}
$$

where $\ell$ is $n$ or $n-1$ according as $n$ is even or odd.

In ( 1.2.6 )–( 1.2.9 ) $k$ and $m$ are nonnegative integers and $z$ is complex.


<a id="E6"></a>
$$
\genfrac{(}{)}{0.0pt}{}{z}{k}=\frac{z(z-1)\cdots(z-k+1)}{k!}=\frac{(-1)^{k}{\left(-z\right)_{k}}}{k!}=(-1)^{k}\genfrac{(}{)}{0.0pt}{}{k-z-1}{k}. \tag{1.2.6}
$$


<a id="E7"></a>
$$
\genfrac{(}{)}{0.0pt}{}{z+1}{k}=\genfrac{(}{)}{0.0pt}{}{z}{k}+\genfrac{(}{)}{0.0pt}{}{z}{k-1}. \tag{1.2.7}
$$


<a id="E8"></a>
$$
\sum^{m}_{k=0}\genfrac{(}{)}{0.0pt}{}{z+k}{k}=\genfrac{(}{)}{0.0pt}{}{z+m+1}{m}. \tag{1.2.8}
$$


<a id="E9"></a>
$$
\genfrac{(}{)}{0.0pt}{}{z}{0}-\genfrac{(}{)}{0.0pt}{}{z}{1}+\dots+(-1)^{m}\genfrac{(}{)}{0.0pt}{}{z}{m}=(-1)^{m}\genfrac{(}{)}{0.0pt}{}{z-1}{m}. \tag{1.2.9}
$$

See also § [26.3](./26.3.md "§26.3 Lattice Paths: Binomial Coefficients ‣ Properties ‣ Chapter 26 Combinatorial Analysis") .


## §1.2(ii) Finite Series

:::{note}
**Notes:**

See Graham et al. ([1994](./bib/G.html#bib974 "Concrete Mathematics: A Foundation for Computer Science"), pp. 31–34); see also Chrystal ([1959a](./bib/C.html#bib500 "Algebra: An Elementary Textbook for the Higher Classes of Secondary Schools and for Colleges"), pp. 482–483, 489).

**See also:**

Annotations for §1.2 and Ch.1
:::


### Arithmetic Progression

:::{note}
**Keywords:**

[arithmetic progression](http://dlmf.nist.gov/search/search?q=arithmetic%20progression)

**See also:**

Annotations for §1.2(ii) , §1.2 and Ch.1
:::


<a id="E10"></a>
$$
a+(a+d)+(a+2d)+\dots+(a+(n-1)d)=na+\tfrac{1}{2}n(n-1)d=\tfrac{1}{2}n(a+\ell), \tag{1.2.10}
$$

where $\ell$ = last term of the series = $a+(n-1)d$ .


### Geometric Progression

:::{note}
**Keywords:**

[geometric progression (or series)](http://dlmf.nist.gov/search/search?q=geometric%20progression%20%28or%20series%29)

**See also:**

Annotations for §1.2(ii) , §1.2 and Ch.1
:::


<a id="E11"></a>
$$
a+ax+ax^{2}+\dots+ax^{n-1}=\frac{a(1-x^{n})}{1-x}, \tag{1.2.11}
$$


## §1.2(iii) Partial Fractions

:::{note}
**Keywords:**

[partial fractions](http://dlmf.nist.gov/search/search?q=partial%20fractions)

**Notes:**

See Graham et al. ([1994](./bib/G.html#bib974 "Concrete Mathematics: A Foundation for Computer Science"), pp. 338–345). See also Chrystal ([1959a](./bib/C.html#bib500 "Algebra: An Elementary Textbook for the Higher Classes of Secondary Schools and for Colleges"), pp. 151–159).

**Referenced by:**

§5.19(i)

**See also:**

Annotations for §1.2 and Ch.1
:::

Let $\alpha_{1},\alpha_{2},\dots,\alpha_{n}$ be distinct constants, and $f(x)$ be a polynomial of degree less than $n$ . Then


<a id="E12"></a>
$$
\frac{f(x)}{(x-\alpha_{1})(x-\alpha_{2})\cdots(x-\alpha_{n})}=\frac{A_{1}}{x-\alpha_{1}}+\frac{A_{2}}{x-\alpha_{2}}+\dots+\frac{A_{n}}{x-\alpha_{n}}, \tag{1.2.12}
$$

where


<a id="E13"></a>
$$
A_{j}=\frac{f(\alpha_{j})}{\prod\limits_{k\not=j}(\alpha_{j}-\alpha_{k})}. \tag{1.2.13}
$$

Also,


<a id="E14"></a>
$$
\frac{f(x)}{(x-\alpha_{1})^{n}}=\frac{B_{1}}{x-\alpha_{1}}+\frac{B_{2}}{(x-\alpha_{1})^{2}}+\dots+\frac{B_{n}}{(x-\alpha_{1})^{n}}, \tag{1.2.14}
$$

where


<a id="E15"></a>
$$
B_{j}=\frac{f^{(n-j)}(\alpha_{1})}{(n-j)!}, \tag{1.2.15}
$$

and $f^{(k)}$ is the $k$ -th derivative of $f$ (§ 1.4(iii) ).

If $m_{1},m_{2},\dots,m_{n}$ are positive integers and $\deg f<\sum_{j=1}^{n}m_{j}$ , then there exist polynomials $f_{j}(x)$ , $\deg f_{j}<m_{j}$ , such that


<a id="E16"></a>
$$
\frac{f(x)}{(x-\alpha_{1})^{m_{1}}(x-\alpha_{2})^{m_{2}}\cdots(x-\alpha_{n})^{m_{n}}}=\frac{f_{1}(x)}{(x-\alpha_{1})^{m_{1}}}+\frac{f_{2}(x)}{(x-\alpha_{2})^{m_{2}}}+\cdots+\frac{f_{n}(x)}{(x-\alpha_{n})^{m_{n}}}. \tag{1.2.16}
$$

To find the polynomials $f_{j}(x)$ , $j=1,2,\dots,n$ , multiply both sides by the denominator of the left-hand side and equate coefficients. See Chrystal ([1959a](./bib/C.html#bib500 "Algebra: An Elementary Textbook for the Higher Classes of Secondary Schools and for Colleges"), pp. 151–159).


## §1.2(iv) Means

:::{note}
**Keywords:**

[arithmetic mean](http://dlmf.nist.gov/search/search?q=arithmetic%20mean) , [geometric mean](http://dlmf.nist.gov/search/search?q=geometric%20mean) , [harmonic mean](http://dlmf.nist.gov/search/search?q=harmonic%20mean) , [means](http://dlmf.nist.gov/search/search?q=means) , [weighted means](http://dlmf.nist.gov/search/search?q=weighted%20means)

**Notes:**

See Hardy et al. ([1967](./bib/H.html#bib1045 "Inequalities"), pp. 12–15).

**Referenced by:**

§1.7(iii)

**See also:**

Annotations for §1.2 and Ch.1
:::

The *arithmetic mean* of $n$ numbers $a_{1},a_{2},\dots,a_{n}$ is


<a id="E17"></a>
$$
A=\frac{a_{1}+a_{2}+\dots+a_{n}}{n}. \tag{1.2.17}
$$

The *geometric mean* $G$ and *harmonic mean* $H$ of $n$ positive numbers $a_{1},a_{2},\dots,a_{n}$ are given by


<a id="E18"></a>
$$
G=(a_{1}a_{2}\cdots a_{n})^{1/n}, \tag{1.2.18}
$$


<a id="E19"></a>
$$
\frac{1}{H}=\frac{1}{n}\left(\frac{1}{a_{1}}+\frac{1}{a_{2}}+\dots+\frac{1}{a_{n}}\right). \tag{1.2.19}
$$

If $r$ is a nonzero real number, then the *weighted mean* $M(r)$ of $n$ nonnegative numbers $a_{1},a_{2},\dots,a_{n}$ , and $n$ positive numbers $p_{1},p_{2},\dots,p_{n}$ with


<a id="E20"></a>
$$
p_{1}+p_{2}+\dots+p_{n}=1, \tag{1.2.20}
$$

is defined by


<a id="E21"></a>
$$
M(r)=(p_{1}a_{1}^{r}+p_{2}a_{2}^{r}+\dots+p_{n}a_{n}^{r})^{1/r}, \tag{1.2.21}
$$

with the exception


<a id="E22"></a>
$$
M(r)=0, \tag{1.2.22}
$$

<a id="EGx1"></a>

$$
\displaystyle\lim_{r\to\infty}M(r) \displaystyle=\max(a_{1},a_{2},\dots,a_{n}), \tag{1.2.23}
$$

:::{note}
**Symbols:**

$n$: nonnegative integer and $M(\NVar{r})$: weighted mean

**A&S Ref:**

3.1.16

**See also:**

Annotations for §1.2(iv) , §1.2 and Ch.1
:::

$$
\displaystyle\lim_{r\to-\infty}M(r) \displaystyle=\min(a_{1},a_{2},\dots,a_{n}). \tag{1.2.24}
$$

:::{note}
**Symbols:**

$n$: nonnegative integer and $M(\NVar{r})$: weighted mean

**A&S Ref:**

3.1.17

**See also:**

Annotations for §1.2(iv) , §1.2 and Ch.1
:::

For $p_{j}=1/n$ , $j=1,2,\dots,n$ ,

<a id="E25"></a>

<a id="Ex1"></a>
$$
\displaystyle M(1) \displaystyle=A, \tag{1.2.25}
$$

<a id="Ex2"></a>
$$
\displaystyle M(-1) \displaystyle=H,
$$

:::{note}
**Symbols:**

$A$: arithmetic mean , $H$: harmonic mean and $M(\NVar{r})$: weighted mean

**A&S Ref:**

3.1.19 3.1.20

**See also:**

Annotations for §1.2(iv) , §1.2 and Ch.1
:::

and


<a id="E26"></a>
$$
\lim_{r\to 0}M(r)=G. \tag{1.2.26}
$$

The last two equations require $a_{j}>0$ for all $j$ .


## §1.2(v) Matrices, Vectors, Scalar Products, and Norms

:::{note}
**Notes:**

See Axler ([2015](./bib/index.html#bib3097 "Linear algebra done right"), Chapters 3c, 7) and Rudin ([1966](./bib/R.html#bib2983 "Real and complex analysis"), Chapter 3).

**Referenced by:**

Erratum (V1.2.0) §1.2 , Version 1.2.0 (March 27, 2024)

**Addition (effective with 1.2.0):**

This subsection was added.

**See also:**

Annotations for §1.2 and Ch.1
:::


### General m×n Matrices

:::{note}
**Keywords:**

[matrix, index notation for m by n](http://dlmf.nist.gov/search/search?q=matrix%2C%20index%20notation%20for%20m%20by%20n)

**See also:**

Annotations for §1.2(v) , §1.2 and Ch.1
:::

The full index form of an $m\times n$ matrix $\mathbf{A}$ is


<a id="E27"></a>
$$
\mathbf{A}=[a_{ij}]=\left[\begin{matrix}a_{11}&a_{12}&\dots&a_{1n}\\
a_{21}&a_{22}&\dots&a_{2n}\\
\vdots&\vdots&\ddots&\vdots\\
a_{m1}&a_{m2}&\dots&a_{mn}\end{matrix}\right], \tag{1.2.27}
$$

with matrix elements $a_{ij}\in\mathbb{C}$ , where $i$ , $j$ are the row and column indices, respectively. A matrix is *zero* if all its elements are zero, denoted $\boldsymbol{{0}}$ . A matrix is *real* if all its elements are real.

The *transpose* of $\mathbf{A}$ = $[a_{ij}]$ is the $n\times m$ matrix


<a id="E28"></a>
$$
\mathbf{A}^{\mathrm{T}}=[a_{ji}], \tag{1.2.28}
$$

the *complex conjugate* is


<a id="E29"></a>
$$
\overline{\mathbf{A}}=[\overline{a_{ij}}], \tag{1.2.29}
$$

the *Hermitian conjugate* is


<a id="E30"></a>
$$
{\mathbf{A}}^{{\rm H}}=[\overline{a_{ji}}]. \tag{1.2.30}
$$

Multiplication by a scalar is given by


<a id="E31"></a>
$$
\alpha\mathbf{A}=\mathbf{A}\alpha=[\alpha a_{ij}]. \tag{1.2.31}
$$

For matrices $\mathbf{A}$ , $\mathbf{B}$ and $\mathbf{C}$ of the same dimensions,


<a id="E32"></a>
$$
\mathbf{A}+\mathbf{B}=\mathbf{B}+\mathbf{A}=[a_{ij}+b_{ij}], \tag{1.2.32}
$$


<a id="E33"></a>
$$
\mathbf{A}+\mathbf{B}+\mathbf{C}=(\mathbf{A}+\mathbf{B})+\mathbf{C}=\mathbf{A}+(\mathbf{B}+\mathbf{C})=[a_{ij}+b_{ij}+c_{ij}]. \tag{1.2.33}
$$


### Multiplication of Matrices

:::{note}
**See also:**

Annotations for §1.2(v) , §1.2 and Ch.1
:::

Multiplication of an $m\times n$ matrix $\mathbf{A}$ and an $m^{\prime}\times n^{\prime}$ matrix $\mathbf{B}$ , giving the $m\times n^{\prime}$ matrix $\mathbf{C}=\mathbf{A}\mathbf{B}$ is *defined* iff $n=m^{\prime}$ . If defined, $\mathbf{C}=[c_{ij}]$ with


<a id="E34"></a>
$$
c_{ij}=\sum_{k=1}^{n}a_{ik}b_{kj}. \tag{1.2.34}
$$

This is the *row times column rule* .

Assuming the indicated multiplications are defined: matrix multiplication is *associative*


<a id="E35"></a>
$$
\mathbf{A}(\mathbf{B}\mathbf{C})=(\mathbf{A}\mathbf{B})\mathbf{C}; \tag{1.2.35}
$$

*distributive* if $\mathbf{B}$ and $\mathbf{C}$ have the same dimensions


<a id="E36"></a>
$$
\mathbf{A}(\mathbf{B}+\mathbf{C})=\mathbf{A}\mathbf{B}+\mathbf{A}\mathbf{C}. \tag{1.2.36}
$$

The *transpose of the product* is


<a id="E37"></a>
$$
(\mathbf{A}\mathbf{B})^{\mathrm{T}}=\mathbf{B}^{\mathrm{T}}\mathbf{A}^{\mathrm{T}}. \tag{1.2.37}
$$

All of the above are defined for $n\times n$ , or *square matrices of order n* , note that matrix multiplication is not necessarily commutative; see § 1.2(vi) for special properties of square matrices.


### Row and Column Vectors

:::{note}
**See also:**

Annotations for §1.2(v) , §1.2 and Ch.1
:::

A *column vector* of *length* $n$ is an $n\times 1$ matrix


<a id="E38"></a>
$$
\mathbf{v}=\left[\begin{matrix}v_{1}\\
v_{2}\\
\vdots\\
v_{n}\end{matrix}\right], \tag{1.2.38}
$$

and the corresponding transposed *row vector* of length $n$ is


<a id="E39"></a>
$$
\mathbf{v}^{\mathrm{T}}=\left[\begin{matrix}v_{1}&v_{2}&\dots&v_{n}\\
\end{matrix}\right]. \tag{1.2.39}
$$

The column vector $\mathbf{v}$ is often written as $[v_{1}~{}v_{2}~{}...~{}v_{n}]^{\mathrm{T}}$ to avoid inconvenient typography. The *zero vector* $\mathbf{v}=\boldsymbol{{0}}$ has $v_{i}=0$ for $i=1,2,\dots,n$ .

Column vectors $\mathbf{u}$ and $\mathbf{v}$ of the same length $n$ have a *scalar product*


<a id="E40"></a>
$$
\left\langle\mathbf{u},\mathbf{v}\right\rangle=\sum_{i=1}^{n}u_{i}\overline{v_{i}}={\mathbf{v}}^{{\rm H}}\mathbf{u}. \tag{1.2.40}
$$

The *dot product* notation $\mathbf{u}\cdot\mathbf{v}$ is reserved for the physical three-dimensional vectors of ( 1.6.2 ).

The scalar product has properties


<a id="E41"></a>
$$
\left\langle\mathbf{u},\mathbf{v}\right\rangle=\overline{\left\langle\mathbf{v},\mathbf{u}\right\rangle}, \tag{1.2.41}
$$

for $\alpha,\beta\in\mathbb{C}$


<a id="E42"></a>
$$
\left\langle\alpha\mathbf{u},\beta\mathbf{v}\right\rangle=\alpha\overline{\beta}\left\langle\mathbf{u},\mathbf{v}\right\rangle, \tag{1.2.42}
$$

and


<a id="E43"></a>
$$
\left\langle\mathbf{v},\mathbf{v}\right\rangle=0, \tag{1.2.43}
$$

if and only if $\mathbf{v}=\boldsymbol{{0}}$ .

If $\mathbf{u}$ , $\mathbf{v}$ , $\alpha$ and $\beta$ are real the complex conjugate bars can be omitted in ( 1.2.40 )–( 1.2.42 ).

Two vectors $\mathbf{u}$ and $\mathbf{v}$ are *orthogonal* if


<a id="E44"></a>
$$
\left\langle\mathbf{u},\mathbf{v}\right\rangle=0. \tag{1.2.44}
$$


### Vector Norms

:::{note}
**See also:**

Annotations for §1.2(v) , §1.2 and Ch.1
:::

The $l^{p}$ norm of a (real or complex) vector is


<a id="E45"></a>
$$
\left\|{\mathbf{v}}\right\|_{p}=\left(\sum_{i=1}^{n}{\left|v_{i}\right|}^{p}\right)^{1/p}, \tag{1.2.45}
$$

Special cases are the *Euclidean length* or *$l^{2}$ norm*


<a id="E46"></a>
$$
\left\|{\mathbf{v}}\right\|=\left\|{\mathbf{v}}\right\|_{2}=\sqrt{\left\langle\mathbf{v},\mathbf{v}\right\rangle}, \tag{1.2.46}
$$

the $l^{1}$ norm


<a id="E47"></a>
$$
\left\|{\mathbf{v}}\right\|_{1}=\sum_{i=1}^{n}\left|v_{i}\right|, \tag{1.2.47}
$$

and as $p\to\infty$


<a id="E48"></a>
$$
\left\|{\mathbf{v}}\right\|_{\infty}=\max(\left|v_{1}\right|,\left|v_{2}\right|,\dots,\left|v_{n}\right|). \tag{1.2.48}
$$

The $l^{2}$ norm is implied unless otherwise indicated. A vector of $l^{2}$ norm unity is *normalized* and every non-zero vector $\mathbf{v}$ can be normalized via $\mathbf{v}\to\mathbf{v}/\left\|{\mathbf{v}}\right\|$ .


### Inequalities

:::{note}
**See also:**

Annotations for §1.2(v) , §1.2 and Ch.1
:::

If


<a id="E49"></a>
$$
\frac{1}{p}+\frac{1}{q}=1 \tag{1.2.49}
$$

we have *Hölder’s Inequality*


<a id="E50"></a>
$$
\left|\left\langle\mathbf{u},\mathbf{v}\right\rangle\right|\leq\left\|{\mathbf{u}}\right\|_{p}\,\left\|{\mathbf{v}}\right\|_{q}, \tag{1.2.50}
$$

which for $p=q=2$ is the *Cauchy-Schwartz inequality*


<a id="E51"></a>
$$
\left|\left\langle\mathbf{u},\mathbf{v}\right\rangle\right|\leq\left\|{\mathbf{u}}\right\|\,\left\|{\mathbf{v}}\right\|, \tag{1.2.51}
$$

the equality holding iff $\mathbf{v}$ is a scalar (real or complex) multiple of $\mathbf{u}$ . The *triangle inequality* ,


<a id="E52"></a>
$$
\left\|{\mathbf{u}+\mathbf{v}}\right\|\leq\left\|{\mathbf{u}}\right\|+\left\|{\mathbf{v}}\right\|. \tag{1.2.52}
$$

For similar and more inequalities see § 1.7(i) .


## §1.2(vi) Square Matrices

:::{note}
**Notes:**

See Axler ([2015](./bib/index.html#bib3097 "Linear algebra done right"), Chapters 6, 10) or Shilov ([2013](./bib/S.html#bib2970 "Introduction to the Theory of Linear Spaces"), Chapters 1, 7).

**Referenced by:**

§1.2(v) , Erratum (V1.2.0) §1.2 , Version 1.2.0 (March 27, 2024)

**Addition (effective with 1.2.0):**

This subsection was added.

**See also:**

Annotations for §1.2 and Ch.1
:::

Square $n\times n$ matrices (said to be of *order $n$* ) dominate the use of matrices in the DLMF, and they have many special properties. Unless otherwise indicated, matrices are assumed square, of order $n$ ; and, when vectors are combined with them, these are of length $n$ .


### Special Forms of Square Matrices

:::{note}
**See also:**

Annotations for §1.2(vi) , §1.2 and Ch.1
:::

The *identity matrix* $\mathbf{I}$ , is defined as


<a id="E53"></a>
$$
\mathbf{I}=[\delta_{i,j}]. \tag{1.2.53}
$$

A matrix $\mathbf{A}$ is: a *diagonal matrix* if


<a id="E54"></a>
$$
a_{ij}=0, \tag{1.2.54}
$$

a *real symmetric matrix* if


<a id="E55"></a>
$$
a_{ji}=a_{ij},~{}~{}a_{ij}\in\mathbb{R}, \tag{1.2.55}
$$

an *Hermitian matrix* if


<a id="E56"></a>
$$
a_{ji}=\overline{a_{ij}},~{}~{}a_{ij}\in\mathbb{C}, \tag{1.2.56}
$$

a *tridiagonal matrix* if


<a id="E57"></a>
$$
a_{ij}=0, \tag{1.2.57}
$$

$\mathbf{A}$ is an *upper* or *lower triangular matrix* if all $a_{ij}$ vanish for $i>j$ or $i<j$ , respectively.

Equation ( 3.2.7 ) displays a *tridiagonal* matrix in index form; ( 3.2.4 ) does the same for a *lower triangular* matrix.


### Special Properties and Definitions Relating to Square Matrices

:::{note}
**See also:**

Annotations for §1.2(vi) , §1.2 and Ch.1
:::


### The Determinant

:::{note}
**Defines:**

$\det$ : determinant

**See also:**

Annotations for §1.2(vi) , §1.2 and Ch.1
:::

The matrix $\mathbf{A}$ has a *determinant* , $\det(\mathbf{A})$ , explored further in § [1.3](./1.3.md "§1.3 Determinants, Linear Operators, and Spectral Expansions ‣ Topics of Discussion ‣ Chapter 1 Algebraic and Analytic Methods") , denoted, in full index form, as


<a id="E58"></a>
$$
\left|\mathbf{A}\right|=\det(\mathbf{A})=\left|\begin{matrix}a_{11}&a_{12}&\dots&a_{1n}\\
a_{21}&a_{22}&\dots&a_{2n}\\
\vdots&\vdots&\ddots&\vdots\\
a_{n1}&a_{n2}&\dots&a_{nn}\end{matrix}\right|, \tag{1.2.58}
$$

where $\det(\mathbf{A})$ is defined by the Leibniz formula


<a id="E59"></a>
$$
\det(\mathbf{A})=\sum_{\sigma\in\mathfrak{S}_{n}}\operatorname{sign}{\sigma}\prod_{i=1}^{n}a_{i,\sigma(i)}. \tag{1.2.59}
$$

$\mathfrak{S}_{n}$ is the set of all permutations of the set $\{1,2,...,n\}$ . See § [26.13](./26.13.md "§26.13 Permutations: Cycle Notation ‣ Properties ‣ Chapter 26 Combinatorial Analysis") for the terminology used herein.


### The Inverse

:::{note}
**Defines:**

${\NVar{\mathbf{A}}}^{-1}$ : matrix inverse

**See also:**

Annotations for §1.2(vi) , §1.2 and Ch.1
:::

If det( $\mathbf{A}$ ) $\neq$ $0$ , $\mathbf{A}$ has a unique *inverse* , ${\mathbf{A}}^{-1}$ , such that


<a id="E60"></a>
$$
\mathbf{A}{\mathbf{A}}^{-1}={\mathbf{A}}^{-1}\mathbf{A}=\mathbf{I}. \tag{1.2.60}
$$

A square matrix $\boldsymbol{{A}}$ is *singular* if $\det(\mathbf{A})=0$ , otherwise it is *non-singular* . If $\det(\mathbf{A})=0$ then $\mathbf{A}\mathbf{B}=\mathbf{A}\mathbf{C}$ does not imply that $\mathbf{B}=\mathbf{C}$ ; if $\det(\mathbf{A})\neq 0$ , then $\mathbf{B}=\mathbf{C}$ , as both sides may be multiplied by ${\mathbf{A}}^{-1}$ .


### n Linear Equations in n Unknowns

:::{note}
**See also:**

Annotations for §1.2(vi) , §1.2 and Ch.1
:::

Given a square matrix $\mathbf{A}$ and a vector $\mathbf{c}$ . If $\det(\mathbf{A})\neq 0$ the *system of $n$ linear equations in $n$ unknowns* ,


<a id="E61"></a>
$$
\mathbf{A}\mathbf{b}=\mathbf{c} \tag{1.2.61}
$$

has a unique solution, $\mathbf{b}={\mathbf{A}}^{-1}\mathbf{c}$ . If $\det(\mathbf{A})=0$ then, depending on $\mathbf{c}$ , there is either no solution or there are infinitely many solutions, being the sum of a particular solution of ( 1.2.61 ) and any solution of $\mathbf{A}\mathbf{b}=\boldsymbol{{0}}$ . Numerical methods and issues for solution of ( 1.2.61 ) appear in §§ 3.2(i) to 3.2(iii) .


### The Trace

:::{note}
**See also:**

Annotations for §1.2(vi) , §1.2 and Ch.1
:::

The *trace* of $\mathbf{A}=[a_{ij}]$ is


<a id="E62"></a>
$$
\operatorname{tr}(\mathbf{A})=\sum_{i=1}^{n}a_{ii}. \tag{1.2.62}
$$

Further,


<a id="E63"></a>
$$
\operatorname{tr}(\alpha\mathbf{A})=\alpha\operatorname{tr}(\mathbf{A}), \tag{1.2.63}
$$


<a id="E64"></a>
$$
\operatorname{tr}(\mathbf{A}+\mathbf{B})=\operatorname{tr}(\mathbf{A})+\operatorname{tr}(\mathbf{B}), \tag{1.2.64}
$$

and


<a id="E65"></a>
$$
\operatorname{tr}(\mathbf{A}\mathbf{B})=\operatorname{tr}(\mathbf{B}\mathbf{A}). \tag{1.2.65}
$$


### The Commutator

:::{note}
**See also:**

Annotations for §1.2(vi) , §1.2 and Ch.1
:::

If $\mathbf{A}\mathbf{B}=\mathbf{B}\mathbf{A}$ the matrices $\mathbf{A}$ and $\mathbf{B}$ are said to *commute* . The difference between $\mathbf{A}\mathbf{B}$ and $\mathbf{B}\mathbf{A}$ is the *commutator* denoted as


<a id="E66"></a>
$$
[{\mathbf{A}},{\mathbf{B}}]=-[{\mathbf{B}},{\mathbf{A}}]=\mathbf{A}\mathbf{B}-\mathbf{B}\mathbf{A}. \tag{1.2.66}
$$


### Norms of Square Matrices

:::{note}
**See also:**

Annotations for §1.2(vi) , §1.2 and Ch.1
:::

Let $\left\|{\mathbf{x}}\right\|=\left\|{\mathbf{x}}\right\|_{2}$ the $l^{2}$ norm, and $\mathbf{E}_{n}$ the space of all $n$ -dimensional vectors. We take $\mathbf{E}_{n}\subset{\mathbb{C}}^{n}$ , but we can also restrict ourselves to vectors and matrices with only real elements. The *norm* of an order $n$ square matrix, $\mathbf{A}$ , is


<a id="E67"></a>
$$
\left\|{\mathbf{A}}\right\|=\max_{\mathbf{x}\in\mathbf{E}_{n}\setminus\left\{\boldsymbol{{0}}\right\}}\frac{\left\|{\mathbf{A}\mathbf{x}}\right\|}{\left\|{\mathbf{x}}\right\|}=\max_{\left\|{\mathbf{x}}\right\|=1}\left\|{\mathbf{A}\mathbf{x}}\right\|. \tag{1.2.67}
$$

Then


<a id="E68"></a>
$$
\left\|{\mathbf{A}\mathbf{B}}\right\|\leq\left\|{\mathbf{A}}\right\|\ \left\|{\mathbf{B}}\right\|, \tag{1.2.68}
$$

and


<a id="E69"></a>
$$
\left\|{\mathbf{A}+\mathbf{B}}\right\|\leq\left\|{\mathbf{A}}\right\|+\left\|{\mathbf{B}}\right\|. \tag{1.2.69}
$$


### Eigenvectors and Eigenvalues of Square Matrices

:::{note}
**See also:**

Annotations for §1.2(vi) , §1.2 and Ch.1
:::

A square matrix $\mathbf{A}$ has an *eigenvalue* $\lambda$ with corresponding *eigenvector* $\mathbf{a}\not=\boldsymbol{{0}}$ if


<a id="E70"></a>
$$
\mathbf{A}\mathbf{a}=\lambda\mathbf{a}. \tag{1.2.70}
$$

Here $\mathbf{a}$ and $\lambda$ may be complex even if $\mathbf{A}$ is real. Eigenvalues are the roots of the polynomial equation


<a id="E71"></a>
$$
\det(\mathbf{A}-\lambda\mathbf{I})=0, \tag{1.2.71}
$$

and for the corresponding eigenvectors one has to solve the linear system


<a id="E72"></a>
$$
(\mathbf{A}-\lambda\mathbf{I})\mathbf{a}=\boldsymbol{{0}}. \tag{1.2.72}
$$

Numerical methods and issues for solution of ( 1.2.72 ) appear in §§ 3.2(iv) to 3.2(vii) .


### Non-Defective Square Matrices

:::{note}
**See also:**

Annotations for §1.2(vi) , §1.2 and Ch.1
:::

Nonzero vectors $\mathbf{v}_{1},\dots,\mathbf{v}_{n}$ are *linearly independent* if $\sum_{i=1}^{n}c_{i}\mathbf{v}_{i}=\boldsymbol{{0}}$ implies that all coefficients $c_{i}$ are zero. A matrix $\mathbf{A}$ of order $n$ is *non-defective* if it has $n$ linearly independent (possibly complex) eigenvectors, otherwise $\mathbf{A}$ is called *defective* . Non-defective matrices are precisely the matrices which can be *diagonalized* via a *similarity transformation* of the form


<a id="E73"></a>
$$
\boldsymbol{{\Lambda}}={\mathbf{S}}^{-1}\mathbf{A}\mathbf{S}. \tag{1.2.73}
$$

The columns of the invertible matrix $\mathbf{S}$ are eigenvectors of $\mathbf{A}$ , and $\boldsymbol{{\Lambda}}$ is a diagonal matrix with the $n$ eigenvalues $\lambda_{i}$ as diagonal elements. The diagonal elements are not necessarily distinct, and the number of identical (degenerate) diagonal elements is the *multiplicity* of that specific eigenvalue. The sum of all multiplicities is $n$ .


### Relation of Eigenvalues to the Determinant and Trace

:::{note}
**See also:**

Annotations for §1.2(vi) , §1.2 and Ch.1
:::

For $\mathbf{A}$ non-defective we obtain from ( 1.2.73 ) and ( 1.3.7 )


<a id="E74"></a>
$$
\det(\mathbf{A})=\det(\mathbf{S}{\mathbf{S}}^{-1}\mathbf{A})=\det({\mathbf{S}}^{-1}\mathbf{A}\mathbf{S})=\prod_{i=1}^{n}\lambda_{i}. \tag{1.2.74}
$$

Thus $\det(\mathbf{A})$ is the product of the $n$ (counted according to their multiplicities) eigenvalues of $\mathbf{A}$ . Similarly, we obtain from ( 1.2.73 ) and ( 1.2.65 )


<a id="E75"></a>
$$
\operatorname{tr}(\mathbf{A})=\operatorname{tr}(\mathbf{S}{\mathbf{S}}^{-1}\mathbf{A})=\operatorname{tr}({\mathbf{S}}^{-1}\mathbf{A}\mathbf{S})=\sum_{i=1}^{n}\lambda_{i}. \tag{1.2.75}
$$

Thus $\operatorname{tr}(\mathbf{A})$ is the sum of the (counted according to their multiplicities) eigenvalues of $\mathbf{A}$ .


### The Matrix Exponential and the Exponential of the Trace

:::{note}
**Keywords:**

[definition](http://dlmf.nist.gov/search/search?q=definition) , [exponential of the trace](http://dlmf.nist.gov/search/search?q=exponential%20of%20the%20trace) , [matrix exponential](http://dlmf.nist.gov/search/search?q=matrix%20exponential)

**See also:**

Annotations for §1.2(vi) , §1.2 and Ch.1
:::

The *matrix exponential* is defined via


<a id="E76"></a>
$$
\exp\left(\mathbf{A}\right)=\sum_{n=0}^{\infty}\frac{1}{n!}\mathbf{A}^{n}, \tag{1.2.76}
$$

which converges, entry-wise or in norm, for all $\mathbf{A}$ .

It follows from ( 1.2.73 ), ( 1.2.74 ) and ( 1.2.75 ) that, for a non-defective matrix $\mathbf{A}$ ,


<a id="E77"></a>
$$
\det(\exp\left(\mathbf{A}\right))=\exp\left(\operatorname{tr}(\mathbf{A})\right)=\operatorname{etr}\left(\mathbf{A}\right). \tag{1.2.77}
$$

Formula ( 1.2.77 ) is more generally valid for all square matrices $\mathbf{A}$ , not necessarily non-defective, see Hall ([2015](./bib/H.html#bib3109 "Lie groups, Lie algebras, and representations"), Thm 2.12).
