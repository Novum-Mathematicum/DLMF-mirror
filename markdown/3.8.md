# §3.8 Nonlinear Equations

:::{note}
**Keywords:**

[computation](http://dlmf.nist.gov/search/search?q=computation) , [iterative methods](http://dlmf.nist.gov/search/search?q=iterative%20methods) , [multiplicity](http://dlmf.nist.gov/search/search?q=multiplicity) , [nonlinear equations](http://dlmf.nist.gov/search/search?q=nonlinear%20equations) , [numerical solutions](http://dlmf.nist.gov/search/search?q=numerical%20solutions) , [of equations](http://dlmf.nist.gov/search/search?q=of%20equations) , [roots](http://dlmf.nist.gov/search/search?q=roots) , [simple](http://dlmf.nist.gov/search/search?q=simple) , [zeros of analytic functions](http://dlmf.nist.gov/search/search?q=zeros%20of%20analytic%20functions)

**Referenced by:**

§29.20(i) , §3.3(v) , Ch.3

**See also:**

Annotations for Ch.3
:::


## §3.8(i) Introduction

:::{note}
**Keywords:**

[acceleration](http://dlmf.nist.gov/search/search?q=acceleration) , [convergence](http://dlmf.nist.gov/search/search?q=convergence) , [cubic](http://dlmf.nist.gov/search/search?q=cubic) , [fixed point](http://dlmf.nist.gov/search/search?q=fixed%20point) , [fixed points](http://dlmf.nist.gov/search/search?q=fixed%20points) , [geometric](http://dlmf.nist.gov/search/search?q=geometric) , [iterative methods](http://dlmf.nist.gov/search/search?q=iterative%20methods) , [linear](http://dlmf.nist.gov/search/search?q=linear) , [local](http://dlmf.nist.gov/search/search?q=local) , [nonlinear equations](http://dlmf.nist.gov/search/search?q=nonlinear%20equations) , [of the $p$th order](http://dlmf.nist.gov/search/search?q=of%20the%20pth%20order) , [quadratic](http://dlmf.nist.gov/search/search?q=quadratic)

**Referenced by:**

§10.74(vi) , §29.20(i) , §3.11(i) , §3.2(iv)

**See also:**

Annotations for §3.8 and Ch.3
:::

The equation to be solved is


<a id="E1"></a>
$$
f(z)=0, \tag{3.8.1}
$$

where $z$ is a real or complex variable and the function $f$ is nonlinear. Solutions are called *roots* of the equation, or *zeros* of $f$ . If $f(z_{0})=0$ and $f^{\prime}(z_{0})\neq 0$ , then $z_{0}$ is a *simple zero* of $f$ . If $f(z_{0})=f^{\prime}(z_{0})=\cdots=f^{(m-1)}(z_{0})=0$ and $f^{(m)}(z_{0})\neq 0$ , then $z_{0}$ is a zero of $f$ of *multiplicity $m$* ; compare § 1.10(i) .

Sometimes the equation takes the form


<a id="E2"></a>
$$
z=\phi(z), \tag{3.8.2}
$$

and the solutions are called *fixed points* of $\phi$ .

Equations ( 3.8.1 ) and ( 3.8.2 ) are usually solved by iterative methods. Let $z_{1},z_{2},\dots$ be a sequence of approximations to a root, or fixed point, $\zeta$ . If


<a id="E3"></a>
$$
\left|z_{n+1}-\zeta\right|<A{\left|z_{n}-\zeta\right|}^{p} \tag{3.8.3}
$$

for all $n$ sufficiently large, where $A$ and $p$ are independent of $n$ , then the sequence is said to have *convergence of the* $p$ th *order* . (More precisely, $p$ is the largest of the possible set of indices for ( 3.8.3 ).) If $p=1$ and $A<1$ , then the convergence is said to be *linear* or *geometric* . If $p=2$ , then the convergence is *quadratic* ; if $p=3$ , then the convergence is *cubic* , and so on.

An iterative method converges *locally* to a solution $\zeta$ if there exists a neighborhood $N$ of $\zeta$ such that $z_{n}\to\zeta$ whenever the initial approximation $z_{0}$ lies within $N$ .


## §3.8(ii) Newton’s Rule

:::{note}
**Keywords:**

[Newton’s rule (or method)](http://dlmf.nist.gov/search/search?q=Newton%20rule%20%28or%20method%29) , [convergence](http://dlmf.nist.gov/search/search?q=convergence) , [iterative methods](http://dlmf.nist.gov/search/search?q=iterative%20methods)

**Notes:**

See Gautschi ([1997a](./bib/G.html#bib895 "Numerical Analysis. An Introduction"), pp. 230–234).

**Referenced by:**

§29.20(iii) , §3.3(v) , §4.45(iii) , §6.18(iii) , §8.25(iii) , §9.17(v)

**See also:**

Annotations for §3.8 and Ch.3
:::

This is an iterative method for real twice-continuously differentiable, or complex analytic, functions:


<a id="E4"></a>
$$
z_{n+1}=z_{n}-\frac{f(z_{n})}{f^{\prime}(z_{n})}, \tag{3.8.4}
$$

If $\zeta$ is a simple zero, then the iteration converges locally and quadratically. For multiple zeros the convergence is linear, but if the multiplicity $m$ is known then quadratic convergence can be restored by multiplying the ratio $f(z_{n})/f^{\prime}(z_{n})$ in ( 3.8.4 ) by $m$ .

For real functions $f(x)$ the sequence of approximations to a real zero $\xi$ will always converge (and converge quadratically) if either:

* $f(x_{0})f^{\prime\prime}(x_{0})>0$ and $f^{\prime}(x)$ , $f^{\prime\prime}(x)$ do not change sign between $x_{0}$ and $\xi$ (monotonic convergence).
* $f(x_{0})f^{\prime\prime}(x_{0})<0$ , $f^{\prime}(x)$ , $f^{\prime\prime}(x)$ do not change sign in the interval $(x_{0},x_{1})$ , and $\xi\in[x_{0},x_{1}]$ (monotonic convergence after the first iteration).


### Example

:::{note}
**See also:**

Annotations for §3.8(ii) , §3.8 and Ch.3
:::

$f(x)=x-\tan x$ . The first positive zero of $f(x)$ lies in the interval $(\pi,\frac{3}{2}\pi)$ ; see Figure 4.15.3 . From this graph we estimate an initial value $x_{0}=4.65$ . Newton’s rule is given by

<a id="E5"></a>

<a id="Ex1"></a>
$$
\displaystyle x_{n+1} \displaystyle=\phi(x_{n}), \tag{3.8.5}
$$

<a id="Ex2"></a>
$$
\displaystyle\phi(x) \displaystyle=x+x{\cot}^{2}x-\cot x.
$$

:::{note}
**Symbols:**

$\cot\NVar{z}$: cotangent function

**See also:**

Annotations for §3.8(ii) , §3.8(ii) , §3.8 and Ch.3
:::

Results appear in Table 3.8.1 . The choice of $x_{0}$ here is critical. When $x_{0}\leq 4.2875$ or $x_{0}\geq 4.7125$ , Newton’s rule does not converge to the required zero. The convergence is faster when we use instead the function $f(x)=x\cos x-\sin x$ ; in addition, the successful interval for the starting value $x_{0}$ is larger.

<a id="T1"></a>
| **$n$** | **$x_{n}$** |
|---|---|
| **0** | **4.65000 00000 000** |
| **1** | **4.60567 66065 900** |
| **2** | **4.55140 53475 751** |
| **3** | **4.50903 76975 617** |
| **4** | **4.49455 61600 185** |
| **5** | **4.49341 56569 391** |
| **6** | **4.49340 94580 903** |
| **7** | **4.49340 94579 091** |
| **8** | **4.49340 94579 091** |
: Table 3.8.1: Newton’s rule for x − tan ⁡ x = 0 .

:::{note}
**Symbols:**

$\tan\NVar{z}$: tangent function

**Referenced by:**

§3.8(ii) , §3.8(iv)

**See also:**

Annotations for §3.8(ii) , §3.8(ii) , §3.8 and Ch.3
:::


## §3.8(iii) Other Methods

:::{note}
**See also:**

Annotations for §3.8 and Ch.3
:::


### Bisection Method

:::{note}
**Keywords:**

[bisection method](http://dlmf.nist.gov/search/search?q=bisection%20method) , [iterative methods](http://dlmf.nist.gov/search/search?q=iterative%20methods)

**Notes:**

See Gautschi ([1997a](./bib/G.html#bib895 "Numerical Analysis. An Introduction"), pp. 217–225), Ostrowski ([1973](./bib/O.html#bib1820 "Solution of Equations in Euclidean and Banach Spaces"), Chapters 3–11), and Traub ([1964](./bib/T.html#bib2267 "Iterative Methods for the Solution of Equations"), pp. 268–269).

**See also:**

Annotations for §3.8(iii) , §3.8 and Ch.3
:::

If $f(a)f(b)<0$ with $a<b$ , then the interval $[a,b]$ contains one or more zeros of $f$ . Bisection of this interval is used to decide where at least one zero is located. All zeros of $f$ in the original interval $[a,b]$ can be computed to any predetermined accuracy. Convergence is slow however; see Kaufman and Lenker ([1986](./bib/K.html#bib1236 "Linear convergence and the bisection algorithm")) and Nievergelt ([1995](./bib/N.html#bib1722 "Bisection hardly ever converges linearly")).


### Regula Falsi

:::{note}
**Keywords:**

[interpolation](http://dlmf.nist.gov/search/search?q=interpolation) , [inverse linear](http://dlmf.nist.gov/search/search?q=inverse%20linear) , [iterative methods](http://dlmf.nist.gov/search/search?q=iterative%20methods) , [regula falsi](http://dlmf.nist.gov/search/search?q=regula%20falsi)

**See also:**

Annotations for §3.8(iii) , §3.8 and Ch.3
:::

Let $x_{0}$ and $x_{1}$ be such that $f_{0}=f(x_{0})$ and $f_{1}=f(x_{1})$ have opposite signs. Inverse linear interpolation (§ 3.3(v) ) is used to obtain the first approximation:


<a id="E6"></a>
$$
x_{2}=x_{1}-\frac{x_{1}-x_{0}}{f_{1}-f_{0}}f_{1}=\frac{f_{1}x_{0}-f_{0}x_{1}}{f_{1}-f_{0}}. \tag{3.8.6}
$$

We continue with $x_{2}$ and either $x_{0}$ or $x_{1}$ , depending which of $f_{0}$ and $f_{1}$ is of opposite sign to $f(x_{2})$ , and so on. The convergence is linear, and again more than one zero may occur in the original interval $[x_{0},x_{1}]$ .


### Secant Method

:::{note}
**Keywords:**

[iterative methods](http://dlmf.nist.gov/search/search?q=iterative%20methods) , [secant method](http://dlmf.nist.gov/search/search?q=secant%20method)

**See also:**

Annotations for §3.8(iii) , §3.8 and Ch.3
:::

Whether or not $f_{0}$ and $f_{1}$ have opposite signs, $x_{2}$ is computed as in ( 3.8.6 ). If the wanted zero $\xi$ is simple, then the method converges locally with order of convergence $p=\frac{1}{2}(1+\sqrt{5})=1.618\ldots\,$ . Because the method requires only one function evaluation per iteration, its numerical efficiency is ultimately higher than that of Newton’s method. There is no guaranteed convergence: the first approximation $x_{2}$ may be outside $[x_{0},x_{1}]$ .


### Steffensen’s Method

:::{note}
**Keywords:**

[Steffensen’s method](http://dlmf.nist.gov/search/search?q=Steffensen%20method) , [iterative methods](http://dlmf.nist.gov/search/search?q=iterative%20methods)

**See also:**

Annotations for §3.8(iii) , §3.8 and Ch.3
:::

This iterative method for solving $z=\phi(z)$ is given by


<a id="E7"></a>
$$
z_{n+1}=z_{n}-\frac{(\phi(z_{n})-z_{n})^{2}}{\phi(\phi(z_{n}))-2\phi(z_{n})+z_{n}}, \tag{3.8.7}
$$

It converges locally and quadratically for both $\mathbb{R}$ and $\mathbb{C}$ .

For other efficient derivative-free methods, see Le ([1985](./bib/L.html#bib1390 "An efficient derivative-free method for solving nonlinear equations")).


### Eigenvalue Methods

:::{note}
**Keywords:**

[eigenvalue methods](http://dlmf.nist.gov/search/search?q=eigenvalue%20methods) , [iterative methods](http://dlmf.nist.gov/search/search?q=iterative%20methods)

**See also:**

Annotations for §3.8(iii) , §3.8 and Ch.3
:::

For the computation of zeros of orthogonal polynomials as eigenvalues of finite tridiagonal matrices (§ 3.5(vi) ), see Gil et al. ([2007a](./bib/G.html#bib935 "Numerical Methods for Special Functions"), pp. 205–207). For the computation of zeros of Bessel functions, Coulomb functions, and conical functions as eigenvalues of finite parts of infinite tridiagonal matrices, see Grad and Zakrajšek ([1973](./bib/G.html#bib969 "Method for evaluation of zeros of Bessel functions")), Ikebe ([1975](./bib/I.html#bib2681 "The zeros of regular Coulomb wave functions and of their derivatives")), Ikebe et al. ([1991](./bib/I.html#bib1122 "Computing zeros and orders of Bessel functions")), Ball ([2000](./bib/B.html#bib182 "Automatic computation of zeros of Bessel functions and other special functions")), and Gil et al. ([2007a](./bib/G.html#bib935 "Numerical Methods for Special Functions"), pp. 205–213).


## §3.8(iv) Zeros of Polynomials

:::{note}
**Keywords:**

[computation](http://dlmf.nist.gov/search/search?q=computation) , [deflation](http://dlmf.nist.gov/search/search?q=deflation) , [explicit formulas](http://dlmf.nist.gov/search/search?q=explicit%20formulas) , [polynomials](http://dlmf.nist.gov/search/search?q=polynomials) , [zeros of polynomials](http://dlmf.nist.gov/search/search?q=zeros%20of%20polynomials)

**Notes:**

For Bairstow’s method see National Physical Laboratory ([1961](./bib/N.html#bib1700 "Modern Computing Methods"), pp. 57–59).

**Referenced by:**

§29.20(i)

**See also:**

Annotations for §3.8 and Ch.3
:::

The polynomial


<a id="E8"></a>
$$
p(z)=a_{n}z^{n}+a_{n-1}z^{n-1}+\dots+a_{0}, \tag{3.8.8}
$$

has $n$ zeros in $\mathbb{C}$ , counting each zero according to its multiplicity. Explicit formulas for the zeros are available if $n\leq 4$ ; see §§ 1.11(iii) and [4.43](./4.43.md "§4.43 Cubic Equations ‣ Applications ‣ Chapter 4 Elementary Functions") . No explicit general formulas exist when $n\geq 5$ .

After a zero $\zeta$ has been computed, the factor $z-\zeta$ is factored out of $p(z)$ as a by-product of Horner’s scheme (§ 1.11(i) ) for the computation of $p(\zeta)$ . In this way polynomials of successively lower degree can be used to find the remaining zeros. (This process is called *deflation* .) However, to guard against the accumulation of rounding errors, a final iteration for each zero should also be performed on the original polynomial $p(z)$ .


### Example

:::{note}
**See also:**

Annotations for §3.8(iv) , §3.8 and Ch.3
:::

$p(z)=z^{4}-1$ . The zeros are $\pm 1$ and $\pm\mathrm{i}$ . Newton’s method is given by

<a id="E9"></a>

<a id="Ex3"></a>
$$
\displaystyle z_{n+1} \displaystyle=\phi(z_{n}), \tag{3.8.9}
$$

<a id="Ex4"></a>
$$
\displaystyle\phi(z) \displaystyle=\frac{3z^{4}+1}{4z^{3}}.
$$

:::{note}
**A&S Ref:**

3.9.6 (in different form)

**Referenced by:**

§3.8(viii)

**See also:**

Annotations for §3.8(iv) , §3.8(iv) , §3.8 and Ch.3
:::

The results for $z_{0}=1.5$ are given in Table 3.8.2 .

<a id="T2"></a>
| **$n$** | **$z_{n}$** |
|---|---|
| **0** | **1.50000 00000 000** |
| **1** | **1.19907 40740 741** |
| **2** | **1.04431 68969 414** |
| **3** | **1.00274 20038 676** |
| **4** | **1.00001 12265 490** |
| **5** | **1.00000 00001 891** |
| **6** | **1.00000 00000 000** |
: Table 3.8.2: Newton’s rule for z 4 − 1 = 0 .

:::{note}
**Referenced by:**

§3.8(iv)

**See also:**

Annotations for §3.8(iv) , §3.8(iv) , §3.8 and Ch.3
:::

As in the case of Table 3.8.1 the quadratic nature of convergence is clearly evident: as the zero is approached, the number of correct decimal places doubles at each iteration.

Newton’s rule can also be used for complex zeros of $p(z)$ . However, when the coefficients are all real, complex arithmetic can be avoided by the following iterative process.


### Bairstow’s Method

:::{note}
**Keywords:**

[Bairstow’s method (for zeros of polynomials)](http://dlmf.nist.gov/search/search?q=Bairstow%20method%20%28for%20zeros%20of%20polynomials%29) , [iterative methods](http://dlmf.nist.gov/search/search?q=iterative%20methods)

**See also:**

Annotations for §3.8(iv) , §3.8 and Ch.3
:::

Let $z^{2}-sz-t$ be an approximation to the real quadratic factor of $p(z)$ that corresponds to a pair of conjugate complex zeros or to a pair of real zeros. We construct sequences $q_{j}$ and $r_{j}$ , $j=n+1,n,\dots,0$ , from $q_{n+1}=r_{n+1}=0$ , $q_{n}=r_{n}=a_{n}$ , and for $j\leq n-1$ ,

<a id="E10"></a>

<a id="Ex5"></a>
$$
\displaystyle q_{j} \displaystyle=a_{j}+sq_{j+1}+tq_{j+2}, \tag{3.8.10}
$$

<a id="Ex6"></a>
$$
\displaystyle r_{j} \displaystyle=q_{j}+sr_{j+1}+tr_{j+2}.
$$

:::{note}
**Defines:**

$r_{j}$ : sequence (locally)

**Symbols:**

$q(x)$: real-valued function

**See also:**

Annotations for §3.8(iv) , §3.8(iv) , §3.8 and Ch.3
:::

Then the next approximation to the quadratic factor is $z^{2}-(s+\Delta s)z-(t+\Delta t)$ , where

<a id="E11"></a>

<a id="Ex7"></a>
$$
\displaystyle\Delta s \displaystyle=\frac{r_{3}q_{0}-r_{2}q_{1}}{r_{2}^{2}-\ell r_{3}}, \tag{3.8.11}
$$

<a id="Ex8"></a>
$$
\displaystyle\Delta t \displaystyle=\frac{\ell q_{1}-r_{2}q_{0}}{r_{2}^{2}-\ell r_{3}},
$$

<a id="Ex9"></a>
$$
\displaystyle\ell \displaystyle=sr_{2}+tr_{3}.
$$

:::{note}
**Symbols:**

$r_{j}$: sequence and $q(x)$: real-valued function

**See also:**

Annotations for §3.8(iv) , §3.8(iv) , §3.8 and Ch.3
:::

The method converges locally and quadratically, except when the wanted quadratic factor is a multiple factor of $q(z)$ . On the last iteration $q_{n}z^{n-2}+q_{n-1}z^{n-3}+\dots+q_{2}$ is the quotient on dividing $p(z)$ by $z^{2}-sz-t$ .


### Example

:::{note}
**Keywords:**

[computation](http://dlmf.nist.gov/search/search?q=computation) , [zeros of polynomials](http://dlmf.nist.gov/search/search?q=zeros%20of%20polynomials)

**See also:**

Annotations for §3.8(iv) , §3.8 and Ch.3
:::

$p(z)=z^{4}-2z^{2}+1$ . With the starting values $s_{0}=\frac{7}{4}$ , $t_{0}=-\frac{1}{2}$ , an approximation to the quadratic factor $z^{2}-2z+1=(z-1)^{2}$ is computed ( $s=2$ , $t=-1$ ). Table 3.8.3 gives the successive values of $s$ and $t$ . The quadratic nature of the convergence is evident.

<a id="T3"></a>
| **$n$** | **$s_{n}$** | **$t_{n}$** |
|---|---|---|
| **0** | **1.75000 00000 000** | **$-$ 0.50000 00000 000** |
| **1** | **2.13527 29454 109** | **$-$ 1.21235 75284 943** |
| **2** | **2.01786 10488 956** | **$-$ 1.02528 61401 539** |
| **3** | **2.00036 06329 466** | **$-$ 1.00047 63067 522** |
| **4** | **2.00000 01474 803** | **$-$ 1.00000 01858 298** |
| **5** | **2.00000 00000 000** | **$-$ 1.00000 00000 000** |
: Table 3.8.3: Bairstow’s method for factoring z 4 − 2 ⁢ z 2 + 1 .

:::{note}
**Referenced by:**

§3.8(iv)

**See also:**

Annotations for §3.8(iv) , §3.8(iv) , §3.8 and Ch.3
:::

This example illustrates the fact that the method succeeds even if the two zeros of the wanted quadratic factor are real and the same.

For further information on the computation of zeros of polynomials see McNamee ([2007](./bib/M.html#bib1588 "Numerical Methods for Roots of Polynomials. Part I")).


## §3.8(v) Zeros of Analytic Functions

:::{note}
**Keywords:**

[Halley’s rule](http://dlmf.nist.gov/search/search?q=Halley%20rule) , [Rouché’s theorem](http://dlmf.nist.gov/search/search?q=Rouch%C3%A9%20theorem) , [iterative methods](http://dlmf.nist.gov/search/search?q=iterative%20methods) , [phase principle](http://dlmf.nist.gov/search/search?q=phase%20principle)

**Notes:**

See Hildebrand ([1974](./bib/H.html#bib1077 "Introduction to Numerical Analysis"), p. 582).

**Referenced by:**

§10.74(vi) , §9.17(v) , Erratum (V1.0.10) for References

**Addition (effective with 1.0.10):**

A sentence was added to refer to Segura ([2013](./bib/S.html#bib2835 "Computing the complex zeros of special functions")) on the distribution of complex zeros for linear homogeneous second-order differential equations.

**See also:**

Annotations for §3.8 and Ch.3
:::

Newton’s rule is the most frequently used iterative process for accurate computation of real or complex zeros of analytic functions $f(z)$ . Another iterative method is *Halley’s rule* :


<a id="E12"></a>
$$
z_{n+1}=z_{n}-\frac{f(z_{n})}{f^{\prime}(z_{n})-(f^{\prime\prime}(z_{n})f(z_{n})/(2f^{\prime}(z_{n})))}. \tag{3.8.12}
$$

This is useful when $f(z)$ satisfies a second-order linear differential equation because of the ease of computing $f^{\prime\prime}(z_{n})$ . The rule converges locally and is cubically convergent.

Initial approximations to the zeros can often be found from asymptotic or other approximations to $f(z)$ , or by application of the phase principle or Rouché’s theorem; see § 1.10(iv) . These results are also useful in ensuring that no zeros are overlooked when the complex plane is being searched.

For an example involving the Airy functions, see Fabijonas and Olver ([1999](./bib/F.html#bib771 "On the reversion of an asymptotic expansion and the zeros of the Airy functions")).

For fixed-point methods for computing zeros of special functions, see Segura ([2002](./bib/S.html#bib2042 "The zeros of special functions from a fixed point method")), Gil and Segura ([2003](./bib/G.html#bib919 "Computing the zeros and turning points of solutions of second order homogeneous linear ODEs")), and Gil et al. ([2007a](./bib/G.html#bib935 "Numerical Methods for Special Functions"), Chapter 7). For describing the distribution of complex zeros of solutions of linear homogeneous second-order differential equations by methods based on the Liouville–Green (WKB) approximation, see Segura ([2013](./bib/S.html#bib2835 "Computing the complex zeros of special functions")).


## §3.8(vi) Conditioning of Zeros

:::{note}
**Keywords:**

[conditioning](http://dlmf.nist.gov/search/search?q=conditioning) , [zeros of analytic functions](http://dlmf.nist.gov/search/search?q=zeros%20of%20analytic%20functions) , [zeros of polynomials](http://dlmf.nist.gov/search/search?q=zeros%20of%20polynomials)

**See also:**

Annotations for §3.8 and Ch.3
:::

Suppose $f(z)$ also depends on a parameter $\alpha$ , denoted by $f(z,\alpha)$ . Then the sensitivity of a simple zero $z$ to changes in $\alpha$ is given by


<a id="E13"></a>
$$
\frac{\mathrm{d}z}{\mathrm{d}\alpha}=-\ifrac{\frac{\partial f}{\partial\alpha}}{\frac{\partial f}{\partial z}}. \tag{3.8.13}
$$

Thus if $f$ is the polynomial ( 3.8.8 ) and $\alpha$ is the coefficient $a_{j}$ , say, then


<a id="E14"></a>
$$
\frac{\mathrm{d}z}{\mathrm{d}a_{j}}=-\frac{z^{j}}{f^{\prime}(z)}. \tag{3.8.14}
$$

For moderate or large values of $n$ it is not uncommon for the magnitude of the right-hand side of ( 3.8.14 ) to be very large compared with unity, signifying that the computation of zeros of polynomials is often an ill-posed problem.


### Example. Wilkinson’s Polynomial

:::{note}
**Keywords:**

[Wilkinson’s](http://dlmf.nist.gov/search/search?q=Wilkinson%E2%80%99s) , [Wilkinson’s polynomial](http://dlmf.nist.gov/search/search?q=Wilkinson%20polynomial) , [computation](http://dlmf.nist.gov/search/search?q=computation) , [iterative methods](http://dlmf.nist.gov/search/search?q=iterative%20methods) , [nonlinear equations](http://dlmf.nist.gov/search/search?q=nonlinear%20equations) , [numerical solutions](http://dlmf.nist.gov/search/search?q=numerical%20solutions) , [polynomials](http://dlmf.nist.gov/search/search?q=polynomials) , [zeros of analytic functions](http://dlmf.nist.gov/search/search?q=zeros%20of%20analytic%20functions)

**See also:**

Annotations for §3.8(vi) , §3.8 and Ch.3
:::

The zeros of


<a id="E15"></a>
$$
p(x)=(x-1)(x-2)\cdots(x-20) \tag{3.8.15}
$$

are well separated but extremely ill-conditioned. Consider $x=20$ and $j=19$ . We have $p^{\mspace{1.0mu}\prime}(20)=19!$ and $a_{19}=1+2+\dots+20=210$ . The perturbation factor ( 3.8.14 ) is given by


<a id="E16"></a>
$$
\frac{\mathrm{d}x}{\mathrm{d}a_{19}}=-\frac{20^{19}}{19!}=(-4.30\dots)\times 10^{7}. \tag{3.8.16}
$$

Corresponding numerical factors in this example for other zeros and other values of $j$ are obtained in Gautschi ([1984](./bib/G.html#bib890 "Questions of Numerical Condition Related to Polynomials"), §4).


## §3.8(vii) Systems of Nonlinear Equations

:::{note}
**Keywords:**

[fixed-point methods](http://dlmf.nist.gov/search/search?q=fixed-point%20methods) , [iterative methods](http://dlmf.nist.gov/search/search?q=iterative%20methods) , [nonlinear equations](http://dlmf.nist.gov/search/search?q=nonlinear%20equations) , [systems](http://dlmf.nist.gov/search/search?q=systems)

**See also:**

Annotations for §3.8 and Ch.3
:::

For fixed-point iterations and Newton’s method for solving systems of nonlinear equations, see Gautschi ([1997a](./bib/G.html#bib895 "Numerical Analysis. An Introduction"), Chapter 4, §9) and Ortega and Rheinboldt ([1970](./bib/O.html#bib1816 "Iterative Solution of Nonlinear Equations in Several Variables")).


## §3.8(viii) Fixed-Point Iterations: Fractals

:::{note}
**Keywords:**

[Julia sets](http://dlmf.nist.gov/search/search?q=Julia%20sets) , [fixed-point methods](http://dlmf.nist.gov/search/search?q=fixed-point%20methods) , [fractals](http://dlmf.nist.gov/search/search?q=fractals) , [iterative methods](http://dlmf.nist.gov/search/search?q=iterative%20methods)

**See also:**

Annotations for §3.8 and Ch.3
:::

The convergence of iterative methods


<a id="E17"></a>
$$
z_{n+1}=\phi(z_{n}), \tag{3.8.17}
$$

for solving fixed-point problems ( 3.8.2 ) cannot always be predicted, especially in the complex plane.

Consider, for example, ( 3.8.9 ). Starting this iteration in the neighborhood of one of the four zeros $\pm 1,\pm\mathrm{i}$ , sequences $\{z_{n}\}$ are generated that converge to these zeros. For an arbitrary starting point $z_{0}\in\mathbb{C}$ , convergence cannot be predicted, and the boundary of the set of points $z_{0}$ that generate a sequence converging to a particular zero has a very complicated structure. It is called a *Julia set* . In general the Julia set of an analytic function $f(z)$ is a *fractal* , that is, a set that is self-similar. See Julia ([1918](./bib/J.html#bib1198 "Memoire sur l’itération des fonctions rationnelles")) and Devaney ([1986](./bib/D.html#bib652 "An Introduction to Chaotic Dynamical Systems")).
